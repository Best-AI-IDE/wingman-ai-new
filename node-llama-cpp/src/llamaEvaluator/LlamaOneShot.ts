import { withLock } from "../utils/withLock.js";
import { ChatPromptWrapper } from "../ChatPromptWrapper.js";
import { AbortError } from "../AbortError.js";
import { GeneralChatPromptWrapper } from "../chatWrappers/GeneralChatPromptWrapper.js";
import { getChatWrapperByBos } from "../chatWrappers/createChatWrapperByBos.js";
import { ConversationInteraction, Token } from "../types.js";
import { generateContextTextFromConversationHistory } from "../chatWrappers/generateContextTextFromConversationHistory.js";
import { removeNullFields } from "../utils/removeNullFields.js";
import { LlamaContext } from "./LlamaContext.js";

const UNKNOWN_UNICODE_CHAR = "\ufffd";

export type LlamaOneShotOptions = {
	context: LlamaContext;
	printLLamaSystemInfo?: boolean;
	systemPrompt?: string;
};

export type LLamaOneShotPromptOptions = {
	onToken?: (tokens: Token[]) => void;
	signal?: AbortSignal;
	maxTokens?: number;

	/**
	 * Temperature is a hyperparameter that controls the randomness of the generated text.
	 * It affects the probability distribution of the model's output tokens.
	 * A higher temperature (e.g., 1.5) makes the output more random and creative,
	 * while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative.
	 * The suggested temperature is 0.8, which provides a balance between randomness and determinism.
	 * At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run.
	 *
	 * Set to `0` to disable.
	 * Disabled by default (set to `0`).
	 */
	temperature?: number;

	/**
	 * Limits the model to consider only the K most likely next tokens for sampling at each step of sequence generation.
	 * An integer number between `1` and the size of the vocabulary.
	 * Set to `0` to disable (which uses the full vocabulary).
	 *
	 * Only relevant when `temperature` is set to a value greater than 0.
	 */
	topK?: number;

	/**
	 * Dynamically selects the smallest set of tokens whose cumulative probability exceeds the threshold P,
	 * and samples the next token only from this set.
	 * A float number between `0` and `1`.
	 * Set to `1` to disable.
	 *
	 * Only relevant when `temperature` is set to a value greater than `0`.
	 */
	topP?: number;

	/**
	 * Trim whitespace from the end of the generated text
	 * Disabled by default.
	 */
	trimWhitespaceSuffix?: boolean;

	repeatPenalty?: false | LlamaOneShotRepeatPenalty;
};

export type LlamaOneShotRepeatPenalty = {
	/**
	 * Number of recent tokens generated by the model to apply penalties to repetition of.
	 * Defaults to `64`.
	 */
	lastTokens?: number;

	punishTokensFilter?: (tokens: Token[]) => Token[];

	/**
	 * Penalize new line tokens.
	 * Enabled by default.
	 */
	penalizeNewLine?: boolean;

	/**
	 * The relative amount to lower the probability of the tokens in `punishTokens` by
	 * Defaults to `1.1`.
	 * Set to `1` to disable.
	 */
	penalty?: number;

	/**
	 * For n time a token is in the `punishTokens` array, lower its probability by `n * frequencyPenalty`
	 * Disabled by default (`0`).
	 * Set to a value between `0` and `1` to enable.
	 */
	frequencyPenalty?: number;

	/**
	 * Lower the probability of all the tokens in the `punishTokens` array by `presencePenalty`
	 * Disabled by default (`0`).
	 * Set to a value between `0` and `1` to enable.
	 */
	presencePenalty?: number;
};

export class LlamaOneShot {
	private readonly _systemPrompt: string;
	private readonly _printLLamaSystemInfo: boolean;
	private _promptIndex: number = 0;
	private _initialized: boolean = false;
	private _lastStopString: string | null = null;
	private _lastStopStringSuffix: string | null = null;
	private readonly _ctx: LlamaContext;

	/**
	 * @param {LlamaOneShotOptions} options
	 */
	public constructor({
		context,
		printLLamaSystemInfo = false,
		systemPrompt = `You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.
            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.
            If you don't know the answer to a question, please don't share false information.`,
	}: LlamaOneShotOptions) {
		this._ctx = context;
		this._printLLamaSystemInfo = printLLamaSystemInfo;
		this._systemPrompt = systemPrompt;
	}

	public get initialized() {
		return this._initialized;
	}

	public get context() {
		return this._ctx;
	}

	public async init() {
		await withLock(this, "init", async () => {
			if (this._initialized) {
				return;
			}

			if (this._printLLamaSystemInfo) {
				console.log("Llama system info");
			}

			this._initialized = true;
		});
	}

	/**
	 * @param {string} prompt
	 * @param {object} options
	 * @returns {Promise<string>}
	 */
	public async prompt(
		prompt: string,
		{
			onToken,
			signal,
			maxTokens,
			temperature,
			topK,
			topP,
			trimWhitespaceSuffix = false,
			repeatPenalty,
		}: LLamaOneShotPromptOptions = {}
	) {
		const { text } = await this.promptWithMeta(prompt, {
			onToken,
			signal,
			maxTokens,
			temperature,
			topK,
			topP,
			trimWhitespaceSuffix,
			repeatPenalty,
		});

		return text;
	}

	/**
	 * @param {string} prompt
	 * @param {LLamaOneShotPromptOptions} options
	 */
	public async promptWithMeta(
		prompt: string,
		{
			onToken,
			signal,
			maxTokens,
			temperature,
			topK,
			topP,
			trimWhitespaceSuffix = false,
			repeatPenalty,
		}: LLamaOneShotPromptOptions = {}
	) {
		if (!this.initialized) {
			await this.init();
		}

		return await withLock(this, "prompt", async () => {
			// const promptText = `You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.
			// ### Instruction:
			// ${prompt}
			// ### Response:`;

			//const promptText = `"<｜fim▁begin｜>${prompt}<|fim_hole|><｜fim▁end｜>"`;

			const promptText = prompt;

			// const promptText = `<s>[INST] <<SYS>>
			// ${this._systemPrompt}
			// <</SYS>>

			// ${prompt} [/INST]`;

			// this._promptWrapper.wrapPrompt(prompt, {
			// 	systemPrompt: this._systemPrompt,
			// 	promptIndex: this._promptIndex,
			// 	lastStopString: this._lastStopString,
			// 	lastStopStringSuffix:
			// 		this._promptIndex === 0
			// 			? this._ctx.prependBos
			// 				? this._ctx.getBosString()
			// 				: null
			// 			: this._lastStopStringSuffix,
			// });

			this._lastStopString = null;
			this._lastStopStringSuffix = null;

			const { text, stopReason, stopString, stopStringSuffix } =
				await this._evalTokens(this._ctx.encode(promptText), {
					onToken,
					signal,
					maxTokens,
					temperature,
					topK,
					topP,
					trimWhitespaceSuffix,
					repeatPenalty:
						repeatPenalty === false
							? { lastTokens: 0 }
							: repeatPenalty,
				});
			this._lastStopString = stopString;
			this._lastStopStringSuffix = stopStringSuffix;

			return {
				text,
				stopReason,
				stopString,
				stopStringSuffix,
			};
		});
	}

	private async _evalTokens(
		tokens: Uint32Array,
		{
			onToken,
			signal,
			maxTokens,
			temperature,
			topK,
			topP,
			trimWhitespaceSuffix = false,
			repeatPenalty: {
				lastTokens: repeatPenaltyLastTokens = 64,
				punishTokensFilter,
				penalizeNewLine,
				penalty,
				frequencyPenalty,
				presencePenalty,
			} = {},
		}: {
			onToken?: (tokens: Token[]) => void;
			signal?: AbortSignal;
			maxTokens?: number;
			temperature?: number;
			topK?: number;
			topP?: number;
			trimWhitespaceSuffix?: boolean;
			repeatPenalty?: LlamaOneShotRepeatPenalty;
		} = {}
	) {
		const stopStrings = [
			`\n\n### Human:`,
			`###`,
			`### Human:`,
			`### Interaction:`,
		];

		const stopStringIndexes: number[] = Array(stopStrings.length).fill(0);
		const skippedChunksQueue: Token[] = [];
		const res: Token[] = [];
		const repeatPenaltyEnabled = repeatPenaltyLastTokens > 0;
		let stopReason: "eosToken" | "stopString" | "maxTokens" = "eosToken";

		const getPenaltyTokens = () => {
			let punishTokens = res.slice(-repeatPenaltyLastTokens);

			if (punishTokensFilter != null)
				punishTokens = punishTokensFilter(punishTokens);

			if (!penalizeNewLine) {
				const nlToken = this.context.getNlToken();

				if (nlToken !== null) {
					punishTokens = punishTokens.filter(
						(token) => token !== nlToken
					);
				}
			}

			return Uint32Array.from(punishTokens);
		};

		const evaluationIterator = this._ctx.evaluate(
			tokens,
			removeNullFields({
				temperature,
				topK,
				topP,
				repeatPenalty: !repeatPenaltyEnabled
					? undefined
					: {
							punishTokens: getPenaltyTokens,
							penalty,
							frequencyPenalty,
							presencePenalty,
					  },
			})
		);

		for await (const chunk of evaluationIterator) {
			if (signal?.aborted) {
				throw new AbortError();
			}
			res.push(chunk);
			onToken?.([chunk]);

			if (maxTokens != null && maxTokens > 0 && res.length >= maxTokens) {
				stopReason = "maxTokens";
				break;
			}
		}

		let resText = this._ctx.decode(Uint32Array.from(res));

		//if (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix)
		resText = resText.trimEnd();

		return {
			text: resText,
			stopReason,
			stopString: null,
			stopStringSuffix: null,
		};
	}

	private _checkStopString(
		tokenStr: string,
		stopStrings: string[],
		stopStringIndexes: number[]
	) {
		let skipTokenEvent = false;

		for (
			let stopStringIndex = 0;
			stopStringIndex < stopStrings.length;
			stopStringIndex++
		) {
			const stopString = stopStrings[stopStringIndex];

			let localShouldSkipTokenEvent = false;
			let i = 0;
			for (
				;
				i < tokenStr.length &&
				stopStringIndexes[stopStringIndex] !== stopString.length;
				i++
			) {
				if (
					tokenStr[i] ===
					stopString[stopStringIndexes[stopStringIndex]]
				) {
					stopStringIndexes[stopStringIndex]++;
					localShouldSkipTokenEvent = true;
				} else {
					stopStringIndexes[stopStringIndex] = 0;
					localShouldSkipTokenEvent = false;
				}
			}

			if (stopStringIndexes[stopStringIndex] === stopString.length) {
				debugger;
				return {
					shouldReturn: true,
					stopString,
					stopStringSuffix:
						tokenStr.length === i ? null : tokenStr.slice(i),
				};
			}

			skipTokenEvent ||= localShouldSkipTokenEvent;
		}

		return { skipTokenEvent };
	}
}
